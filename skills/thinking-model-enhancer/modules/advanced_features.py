#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€ç»´æ¨¡å‹å¢å¼ºå™¨ - é«˜çº§åŠŸèƒ½æ¨¡å—
Thinking Model Enhancer - Advanced Features Module

å®ç°é«˜çº§åŠŸèƒ½ï¼šæ€§èƒ½è¿½è¸ªã€æ€ç»´å¯è§†åŒ–ã€æ‰¹é‡å¤„ç†ã€è‡ªå®šä¹‰é…ç½®
"""

import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from collections import defaultdict
import statistics


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    total_runs: int = 0
    successful_runs: int = 0
    failed_runs: int = 0
    total_duration_ms: float = 0
    avg_duration_ms: float = 0
    min_duration_ms: float = float('inf')
    max_duration_ms: float = 0
    by_model: Dict[str, Dict] = field(default_factory=dict)
    by_problem_type: Dict[str, Dict] = field(default_factory=dict)
    recent_runs: List[Dict] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            "total_runs": self.total_runs,
            "successful_runs": self.successful_runs,
            "failed_runs": self.failed_runs,
            "success_rate": self.successful_runs / self.total_runs if self.total_runs > 0 else 0,
            "total_duration_ms": self.total_duration_ms,
            "avg_duration_ms": self.avg_duration_ms,
            "min_duration_ms": self.min_duration_ms if self.min_duration_ms != float('inf') else 0,
            "max_duration_ms": self.max_duration_ms,
            "by_model": self.by_model,
            "by_problem_type": self.by_problem_type,
            "recent_runs": self.recent_runs[-50:]  # æœ€è¿‘50æ¡
        }


class PerformanceTracker:
    """æ€§èƒ½è¿½è¸ªå™¨"""
    
    def __init__(self, storage_dir: Optional[str] = None):
        """åˆå§‹åŒ–æ€§èƒ½è¿½è¸ªå™¨"""
        if storage_dir is None:
            self.storage_dir = Path.home() / ".claude" / "thinking_models" / "performance"
        else:
            self.storage_dir = Path(storage_dir)
        
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.metrics_file = self.storage_dir / "metrics.json"
        
        # åŠ è½½ç°æœ‰æŒ‡æ ‡
        self.metrics = self._load_metrics()
        
        # å®æ—¶è¿½è¸ª
        self.current_run_start: Optional[float] = None
        self.current_run_data: Dict = {}
    
    def _load_metrics(self) -> PerformanceMetrics:
        """åŠ è½½æ€§èƒ½æŒ‡æ ‡"""
        if self.metrics_file.exists():
            with open(self.metrics_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                metrics = PerformanceMetrics()
                metrics.total_runs = data.get('total_runs', 0)
                metrics.successful_runs = data.get('successful_runs', 0)
                metrics.failed_runs = data.get('failed_runs', 0)
                metrics.total_duration_ms = data.get('total_duration_ms', 0)
                metrics.avg_duration_ms = data.get('avg_duration_ms', 0)
                metrics.min_duration_ms = data.get('min_duration_ms', 0)
                metrics.max_duration_ms = data.get('max_duration_ms', 0)
                metrics.by_model = data.get('by_model', {})
                metrics.by_problem_type = data.get('by_problem_type', {})
                metrics.recent_runs = data.get('recent_runs', [])
                return metrics
        return PerformanceMetrics()
    
    def _save_metrics(self):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡"""
        with open(self.metrics_file, 'w', encoding='utf-8') as f:
            json.dump(self.metrics.to_dict(), f, ensure_ascii=False, indent=2)
    
    def start_run(self, model: str, problem_type: str, problem: str):
        """å¼€å§‹ä¸€æ¬¡è¿è¡Œè¿½è¸ª"""
        self.current_run_start = time.time()
        self.current_run_data = {
            "model": model,
            "problem_type": problem_type,
            "problem": problem[:100],
            "start_time": datetime.now().isoformat(),
            "stages": []
        }
    
    def log_stage(self, stage_name: str, duration_ms: float):
        """è®°å½•é˜¶æ®µä¿¡æ¯"""
        if self.current_run_data:
            self.current_run_data["stages"].append({
                "stage": stage_name,
                "duration_ms": duration_ms
            })
    
    def end_run(self, success: bool, result_summary: str = ""):
        """ç»“æŸè¿è¡Œè¿½è¸ª"""
        if self.current_run_start is None:
            return
        
        duration_ms = (time.time() - self.current_run_start) * 1000
        
        # æ›´æ–°æ€»ä½“æŒ‡æ ‡
        self.metrics.total_runs += 1
        if success:
            self.metrics.successful_runs += 1
        else:
            self.metrics.failed_runs += 1
        
        self.metrics.total_duration_ms += duration_ms
        self.metrics.avg_duration_ms = self.metrics.total_duration_ms / self.metrics.total_runs
        self.metrics.min_duration_ms = min(self.metrics.min_duration_ms, duration_ms)
        self.metrics.max_duration_ms = max(self.metrics.max_duration_ms, duration_ms)
        
        # æ›´æ–°æ¨¡å‹æŒ‡æ ‡
        model = self.current_run_data.get("model", "unknown")
        if model not in self.metrics.by_model:
            self.metrics.by_model[model] = {
                "runs": 0, "success": 0, "total_duration": 0
            }
        self.metrics.by_model[model]["runs"] += 1
        if success:
            self.metrics.by_model[model]["success"] += 1
        self.metrics.by_model[model]["total_duration"] += duration_ms
        
        # æ›´æ–°é—®é¢˜ç±»å‹æŒ‡æ ‡
        problem_type = self.current_run_data.get("problem_type", "unknown")
        if problem_type not in self.metrics.by_problem_type:
            self.metrics.by_problem_type[problem_type] = {
                "runs": 0, "success": 0
            }
        self.metrics.by_problem_type[problem_type]["runs"] += 1
        if success:
            self.metrics.by_problem_type[problem_type]["success"] += 1
        
        # æ·»åŠ åˆ°æœ€è¿‘è®°å½•
        self.metrics.recent_runs.append({
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "problem_type": problem_type,
            "duration_ms": duration_ms,
            "success": success,
            "stages": len(self.current_run_data.get("stages", [])),
            "summary": result_summary[:50]
        })
        
        # ä¿å­˜å¹¶é‡ç½®
        self._save_metrics()
        self.current_run_start = None
        self.current_run_data = {}
    
    def get_metrics(self) -> PerformanceMetrics:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics
    
    def get_model_performance(self, model: str) -> Dict:
        """è·å–ç‰¹å®šæ¨¡å‹çš„æ€§èƒ½"""
        return self.metrics.by_model.get(model, {
            "runs": 0, "success": 0, "total_duration": 0
        })
    
    def get_summary_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦æŠ¥å‘Š"""
        m = self.metrics
        success_rate = m.successful_runs / m.total_runs if m.total_runs > 0 else 0
        
        lines = [
            "ğŸ“Š æ€ç»´æ¨¡å‹æ€§èƒ½æŠ¥å‘Š",
            "=" * 50,
            "",
            f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:",
            f"   æ€»è¿è¡Œæ¬¡æ•°: {m.total_runs}",
            f"   æˆåŠŸ: {m.successful_runs} | å¤±è´¥: {m.failed_runs}",
            f"   æˆåŠŸç‡: {success_rate*100:.1f}%",
            f"   å¹³å‡è€—æ—¶: {m.avg_duration_ms:.1f}ms",
            f"   æœ€çŸ­è€—æ—¶: {m.min_duration_ms:.1f}ms" if m.min_duration_ms != float('inf') else "",
            f"   æœ€é•¿è€—æ—¶: {m.max_duration_ms:.1f}ms",
        ]
        
        if m.by_model:
            lines.extend(["", f"ğŸ“‚ æŒ‰æ¨¡å‹:"])
            for model, data in sorted(m.by_model.items(), key=lambda x: x[1]["runs"], reverse=True):
                model_success = data["success"] / data["runs"] if data["runs"] > 0 else 0
                lines.append(f"   â€¢ {model}: {data['runs']}æ¬¡ (æˆåŠŸç‡{ model_success*100:.0f}%)")
        
        if m.by_problem_type:
            lines.extend(["", f"ğŸ·ï¸ æŒ‰é—®é¢˜ç±»å‹:"])
            for ptype, data in sorted(m.by_problem_type.items(), key=lambda x: x[1]["runs"], reverse=True):
                lines.append(f"   â€¢ {ptype}: {data['runs']}æ¬¡")
        
        return "\n".join(filter(None, lines))


class ThinkingVisualizer:
    """æ€ç»´å¯è§†åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯è§†åŒ–å™¨"""
        pass
    
    def format_thinking_process(self, 
                                 problem: str,
                                 analysis: Dict,
                                 model: str,
                                 stages: List[Dict],
                                 result: Dict) -> str:
        """
        æ ¼å¼åŒ–æ€ç»´è¿‡ç¨‹ä¸ºå¯è¯»æŠ¥å‘Š
        
        Args:
            problem: é—®é¢˜æè¿°
            analysis: é—®é¢˜åˆ†æç»“æœ
            model: ä½¿ç”¨çš„æ¨¡å‹
            stages: å„é˜¶æ®µå¤„ç†ç»“æœ
            result: æœ€ç»ˆç»“æœ
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        lines = [
            f"ğŸ§  æ€ç»´è¿‡ç¨‹æŠ¥å‘Š",
            "=" * 60,
            "",
            f"ğŸ“ é—®é¢˜: {problem[:100]}{'...' if len(problem) > 100 else ''}",
            "",
            f"ğŸ” é—®é¢˜åˆ†æ:",
            f"   ç±»å‹: {analysis.get('type', 'æœªçŸ¥')}",
            f"   å¤æ‚åº¦: {analysis.get('complexity', 'N/A')}/10",
            f"   ç½®ä¿¡åº¦: {analysis.get('confidence', 'N/A')}",
            "",
            f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {model}",
            "",
            f"ğŸ”„ å¤„ç†è¿‡ç¨‹ ({len(stages)} é˜¶æ®µ):"
        ]
        
        for i, stage in enumerate(stages, 1):
            lines.append(f"   {i}. {stage.get('name', 'Unknown')}")
            if stage.get("findings"):
                for finding in stage["findings"][:3]:
                    lines.append(f"      â€¢ {finding}")
            if stage.get("duration_ms"):
                lines.append(f"      â±ï¸ {stage['duration_ms']:.1f}ms")
        
        lines.extend([
            "",
            f"âœ… å¤„ç†ç»“æœ:",
            f"   ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}",
            f"   è€—æ—¶: {result.get('duration_ms', 0):.1f}ms",
        ])
        
        if result.get("recommendations"):
            lines.extend([
                "",
                f"ğŸ’¡ å»ºè®®:",
                *[f"   â€¢ {rec}" for rec in result["recommendations"][:5]]
            ])
        
        return "\n".join(lines)
    
    def generate_text_chart(self, 
                            data: List[Dict], 
                            value_key: str = "success_rate",
                            label_key: str = "label") -> str:
        """
        ç”Ÿæˆç®€å•çš„æ–‡æœ¬å›¾è¡¨
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            value_key: æ•°å€¼å­—æ®µå
            label_key: æ ‡ç­¾å­—æ®µå
            
        Returns:
            æ–‡æœ¬å›¾è¡¨å­—ç¬¦ä¸²
        """
        if not data:
            return "æš‚æ— æ•°æ® ğŸ“Š"
        
        max_value = max(d.get(value_key, 0) for d in data)
        max_label_len = max(len(d.get(label_key, "")) for d in data)
        
        lines = []
        for item in data:
            label = item.get(label_key, "")[:20]
            value = item.get(value_key, 0)
            
            # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
            bar_length = int((value / max_value) * 20) if max_value > 0 else 0
            bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            
            # æ ¼å¼åŒ–è¡Œ
            if isinstance(value, float):
                if value > 1:
                    line = f"{label:<{max_label_len}} {bar} {value:.0f}"
                else:
                    line = f"{label:<{max_label_len}} {bar} {value*100:.1f}%"
            else:
                line = f"{label:<{max_label_len}} {bar} {value}"
            
            lines.append(line)
        
        return "\n".join(lines)


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, 
                 processor_func: Callable,
                 max_concurrent: int = 3,
                 timeout_seconds: int = 60):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            processor_func: å¤„ç†å‡½æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            timeout_seconds: è¶…æ—¶æ—¶é—´
        """
        self.processor = processor_func
        self.max_concurrent = max_concurrent
        self.timeout = timeout_seconds
        
        self.results: List[Dict] = []
        self.stats = {
            "total": 0,
            "completed": 0,
            "failed": 0,
            "start_time": None,
            "end_time": None
        }
    
    def process(self, items: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†é¡¹ç›®
        
        Args:
            items: é¡¹ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹ç›®åŒ…å«idå’Œdata
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        self.stats["total"] = len(items)
        self.stats["start_time"] = datetime.now().isoformat()
        
        results = []
        for item in items:
            try:
                result = self.processor(item["data"])
                results.append({
                    "id": item.get("id", ""),
                    "status": "success",
                    "result": result
                })
                self.stats["completed"] += 1
            except Exception as e:
                results.append({
                    "id": item.get("id", ""),
                    "status": "failed",
                    "error": str(e)
                })
                self.stats["failed"] += 1
        
        self.stats["end_time"] = datetime.now().isoformat()
        self.results = results
        
        return results
    
    def get_stats(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡"""
        duration = None
        if self.stats["start_time"] and self.stats["end_time"]:
            start = datetime.fromisoformat(self.stats["start_time"])
            end = datetime.fromisoformat(self.stats["end_time"])
            duration = (end - start).total_seconds()
        
        return {
            **self.stats,
            "duration_seconds": duration,
            "success_rate": self.stats["completed"] / self.stats["total"] if self.stats["total"] > 0 else 0
        }


class ModelConfigManager:
    """æ¨¡å‹é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: Optional[str] = None):
        """åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨"""
        if config_dir is None:
            self.config_dir = Path.home() / ".claude" / "thinking_models" / "config"
        else:
            self.config_dir = Path(config_dir)
        
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.config_file = self.config_dir / "models.json"
        
        # åŠ è½½æˆ–åˆ›å»ºé»˜è®¤é…ç½®
        self.config = self._load_or_create_config()
    
    def _load_or_create_config(self) -> Dict:
        """åŠ è½½æˆ–åˆ›å»ºé»˜è®¤é…ç½®"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # é»˜è®¤é…ç½®
        default_config = {
            "version": "1.0",
            "updated_at": datetime.now().isoformat(),
            "models": {
                "research_mode": {
                    "enabled": True,
                    "timeout_seconds": 120,
                    "max_retries": 3,
                    "priority": 1
                },
                "diagnostic_mode": {
                    "enabled": True,
                    "timeout_seconds": 90,
                    "max_retries": 2,
                    "priority": 2
                },
                "generic_pipeline": {
                    "enabled": True,
                    "timeout_seconds": 60,
                    "max_retries": 3,
                    "priority": 10
                }
            },
            "general": {
                "auto_store_results": True,
                "confidence_threshold": 0.6,
                "enable_visualization": True,
                "max_history_items": 100
            }
        }
        
        self._save_config(default_config)
        return default_config
    
    def _save_config(self, config: Dict):
        """ä¿å­˜é…ç½®"""
        config["updated_at"] = datetime.now().isoformat()
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    def get_model_config(self, model_name: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹é…ç½®"""
        return self.config.get("models", {}).get(model_name)
    
    def set_model_config(self, model_name: str, settings: Dict):
        """è®¾ç½®æ¨¡å‹é…ç½®"""
        if model_name not in self.config["models"]:
            self.config["models"][model_name] = {}
        
        self.config["models"][model_name].update(settings)
        self._save_config(self.config)
    
    def get_general_config(self) -> Dict:
        """è·å–é€šç”¨é…ç½®"""
        return self.config.get("general", {})
    
    def set_general_config(self, settings: Dict):
        """è®¾ç½®é€šç”¨é…ç½®"""
        self.config["general"].update(settings)
        self._save_config(self.config)
    
    def is_model_enabled(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        model_config = self.get_model_config(model_name)
        return model_config.get("enabled", True) if model_config else True


def get_performance_tracker() -> PerformanceTracker:
    """è·å–æ€§èƒ½è¿½è¸ªå™¨å®ä¾‹"""
    return PerformanceTracker()


def get_visualizer() -> ThinkingVisualizer:
    """è·å–å¯è§†åŒ–å™¨å®ä¾‹"""
    return ThinkingVisualizer()


def get_batch_processor(processor_func: Callable) -> BatchProcessor:
    """è·å–æ‰¹é‡å¤„ç†å™¨å®ä¾‹"""
    return BatchProcessor(processor_func)


def get_config_manager() -> ModelConfigManager:
    """è·å–é…ç½®ç®¡ç†å™¨å®ä¾‹"""
    return ModelConfigManager()
