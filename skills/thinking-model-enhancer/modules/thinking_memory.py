#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€ç»´æ¨¡å‹å¢å¼ºå™¨ - è®°å¿†ç³»ç»Ÿé›†æˆæ¨¡å—
Thinking Model Enhancer - Memory System Integration Module

å®ç°ä¸è®°å¿†ç³»ç»Ÿçš„æŸ¥è¯¢ã€å­˜å‚¨å’Œæ¯”è¾ƒåŠŸèƒ½ï¼Œæ”¯æŒå†å²æ€ç»´æ¨¡å‹çš„æŒä¹…åŒ–å’Œæ£€ç´¢ã€‚
"""

import json
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import hashlib
import re


@dataclass
class ModelSnapshot:
    """æ€ç»´æ¨¡å‹å¿«ç…§"""
    snapshot_id: str
    model_type: str
    problem_summary: str
    input_hash: str
    output_summary: str
    success: bool
    feedback_score: Optional[int]  # 1-5 ç”¨æˆ·åé¦ˆ
    timestamp: str
    duration_ms: float
    stages_used: List[str]
    key_findings: List[str]
    user_rating: Optional[int] = None
    
    def to_dict(self) -> Dict:
        return {
            "snapshot_id": self.snapshot_id,
            "model_type": self.model_type,
            "problem_summary": self.problem_summary,
            "input_hash": self.input_hash,
            "output_summary": self.output_summary,
            "success": self.success,
            "feedback_score": self.feedback_score,
            "timestamp": self.timestamp,
            "duration_ms": self.duration_ms,
            "stages_used": self.stages_used,
            "key_findings": self.key_findings,
            "user_rating": self.user_rating
        }


class ThinkingMemory:
    """æ€ç»´æ¨¡å‹è®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, memory_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ€ç»´è®°å¿†ç®¡ç†å™¨
        
        Args:
            memory_dir: è®°å¿†å­˜å‚¨ç›®å½•
        """
        if memory_dir is None:
            self.memory_dir = Path.home() / ".claude" / "thinking_models" / "memory"
        else:
            self.memory_dir = Path(memory_dir)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç´¢å¼•æ–‡ä»¶
        self.index_file = self.memory_dir / "model_index.json"
        self._init_index()
    
    def _init_index(self):
        """åˆå§‹åŒ–è®°å¿†ç´¢å¼•"""
        if not self.index_file.exists():
            index_data = {
                "last_updated": datetime.now().isoformat(),
                "total_snapshots": 0,
                "by_type": {},
                "by_success": {"success": 0, "failed": 0},
                "avg_rating": 0,
                "frequent_problems": []
            }
            self._save_index(index_data)
    
    def _load_index(self) -> Dict:
        """åŠ è½½è®°å¿†ç´¢å¼•"""
        if self.index_file.exists():
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return self._init_index() or {"last_updated": "", "total_snapshots": 0}
    
    def _save_index(self, index_data: Dict):
        """ä¿å­˜è®°å¿†ç´¢å¼•"""
        index_data["last_updated"] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    def _generate_input_hash(self, input_text: str) -> str:
        """ç”Ÿæˆè¾“å…¥çš„å“ˆå¸Œå€¼"""
        # æ ‡å‡†åŒ–è¾“å…¥
        normalized = re.sub(r'\s+', '', input_text.lower())
        return hashlib.md5(normalized.encode()).hexdigest()[:12]
    
    def store_snapshot(self, snapshot: ModelSnapshot) -> bool:
        """
        å­˜å‚¨æ€ç»´æ¨¡å‹å¿«ç…§
        
        Args:
            snapshot: ModelSnapshot å¯¹è±¡
            
        Returns:
            æ˜¯å¦å­˜å‚¨æˆåŠŸ
        """
        try:
            # ç”Ÿæˆå¿«ç…§IDï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not snapshot.snapshot_id:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                snapshot.snapshot_id = f"{snapshot.model_type}_{timestamp}_{snapshot.input_hash}"
            
            # ä¿å­˜å¿«ç…§æ–‡ä»¶
            snapshot_file = self.memory_dir / f"{snapshot.snapshot_id}.json"
            with open(snapshot_file, 'w', encoding='utf-8') as f:
                json.dump(snapshot.to_dict(), f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°ç´¢å¼•
            index = self._load_index()
            index["total_snapshots"] += 1
            
            # æ›´æ–°ç±»å‹ç»Ÿè®¡
            if snapshot.model_type not in index["by_type"]:
                index["by_type"][snapshot.model_type] = 0
            index["by_type"][snapshot.model_type] += 1
            
            # æ›´æ–°æˆåŠŸç‡
            if snapshot.success:
                index["by_success"]["success"] += 1
            else:
                index["by_success"]["failed"] += 1
            
            # æ›´æ–°è¯„åˆ†
            if snapshot.user_rating:
                current_avg = index.get("avg_rating", 0)
                count = index["by_type"].get(snapshot.model_type, 1)
                index["avg_rating"] = (current_avg * (count - 1) + snapshot.user_rating) / count
            
            self._save_index(index)
            
            return True
        except Exception as e:
            print(f"å­˜å‚¨å¿«ç…§å¤±è´¥: {e}")
            return False
    
    def query_similar_problems(self, 
                               query: str, 
                               model_type: Optional[str] = None,
                               limit: int = 5) -> List[Dict]:
        """
        æŸ¥è¯¢ç›¸ä¼¼é—®é¢˜çš„å†å²è®°å½•
        
        Args:
            query: æŸ¥è¯¢é—®é¢˜
            model_type: å¯é€‰ï¼ŒæŒ‰æ¨¡å‹ç±»å‹è¿‡æ»¤
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            ç›¸ä¼¼é—®é¢˜çš„å†å²è®°å½•åˆ—è¡¨
        """
        query_hash = self._generate_input_hash(query)
        query_keywords = set(re.findall(r'[\w]+', query.lower()))
        
        results = []
        
        # éå†æ‰€æœ‰å¿«ç…§æ–‡ä»¶
        for snapshot_file in self.memory_dir.glob("*.json"):
            if snapshot_file.name == "model_index.json":
                continue
            
            try:
                with open(snapshot_file, 'r', encoding='utf-8') as f:
                    snapshot = json.load(f)
                
                # æŒ‰æ¨¡å‹ç±»å‹è¿‡æ»¤
                if model_type and snapshot.get("model_type") != model_type:
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                snapshot_keywords = set(re.findall(r'[\w]+', 
                    snapshot.get("problem_summary", "").lower()))
                
                # è®¡ç®—å…³é”®è¯é‡å 
                overlap = len(query_keywords & snapshot_keywords)
                
                if overlap > 0:
                    results.append({
                        "snapshot": snapshot,
                        "similarity_score": overlap,
                        "match_type": "keyword_overlap"
                    })
                    
            except Exception:
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        return [r["snapshot"] for r in results[:limit]]
    
    def get_model_statistics(self, model_type: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
        
        Args:
            model_type: å¯é€‰ï¼ŒæŒ‰æ¨¡å‹ç±»å‹è¿‡æ»¤
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        index = self._load_index()
        
        stats = {
            "total_snapshots": index["total_snapshots"],
            "by_type": index["by_type"],
            "success_rate": 0,
            "avg_rating": index.get("avg_rating", 0)
        }
        
        # è®¡ç®—æˆåŠŸç‡
        total = index["by_success"]["success"] + index["by_success"]["failed"]
        if total > 0:
            stats["success_rate"] = index["by_success"]["success"] / total
        
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ç±»å‹
        if model_type:
            model_stats = self._get_model_detail_stats(model_type)
            stats.update(model_stats)
        
        return stats
    
    def _get_model_detail_stats(self, model_type: str) -> Dict:
        """è·å–ç‰¹å®šæ¨¡å‹çš„è¯¦ç»†ç»Ÿè®¡"""
        snapshots = []
        
        for snapshot_file in self.memory_dir.glob("*.json"):
            if snapshot_file.name == "model_index.json":
                continue
            
            try:
                with open(snapshot_file, 'r', encoding='utf-8') as f:
                    snapshot = json.load(f)
                    if snapshot.get("model_type") == model_type:
                        snapshots.append(snapshot)
            except Exception:
                continue
        
        if not snapshots:
            return {"model_type": model_type, "count": 0}
        
        # è®¡ç®—ç»Ÿè®¡
        ratings = [s.get("user_rating") for s in snapshots if s.get("user_rating")]
        successes = [s for s in snapshots if s.get("success")]
        
        return {
            "model_type": model_type,
            "count": len(snapshots),
            "success_count": len(successes),
            "success_rate": len(successes) / len(snapshots) if snapshots else 0,
            "avg_rating": sum(ratings) / len(ratings) if ratings else 0,
            "avg_duration_ms": sum(s.get("duration_ms", 0) for s in snapshots) / len(snapshots)
        }
    
    def get_recent_snapshots(self, days: int = 7, limit: int = 10) -> List[Dict]:
        """
        è·å–æœ€è¿‘çš„å¿«ç…§
        
        Args:
            days: å¤©æ•°
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            æœ€è¿‘çš„å¿«ç…§åˆ—è¡¨
        """
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        results = []
        
        for snapshot_file in self.memory_dir.glob("*.json"):
            if snapshot_file.name == "model_index.json":
                continue
            
            try:
                with open(snapshot_file, 'r', encoding='utf-8') as f:
                    snapshot = json.load(f)
                    
                    if snapshot.get("timestamp", "") >= cutoff_date:
                        results.append(snapshot)
                        
            except Exception:
                continue
        
        # æŒ‰æ—¶é—´æ’åº
        results.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
        
        return results[:limit]
    
    def clear_old_snapshots(self, days: int = 90) -> int:
        """
        æ¸…ç†æ—§å¿«ç…§
        
        Args:
            days: è¶…è¿‡å¤šå°‘å¤©çš„å¿«ç…§è¢«æ¸…ç†
            
        Returns:
            æ¸…ç†çš„å¿«ç…§æ•°é‡
        """
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        cleared_count = 0
        
        for snapshot_file in self.memory_dir.glob("*.json"):
            if snapshot_file.name == "model_index.json":
                continue
            
            try:
                with open(snapshot_file, 'r', encoding='utf-8') as f:
                    snapshot = json.load(f)
                    
                if snapshot.get("timestamp", "") < cutoff_date:
                    snapshot_file.unlink()
                    cleared_count += 1
                    
            except Exception:
                continue
        
        return cleared_count
    
    def compare_with_history(self, 
                             current_problem: str,
                             model_type: str,
                             limit: int = 3) -> Dict[str, Any]:
        """
        å°†å½“å‰é—®é¢˜ä¸å†å²æ¡ˆä¾‹æ¯”è¾ƒ
        
        Args:
            current_problem: å½“å‰é—®é¢˜
            model_type: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
            limit: æ¯”è¾ƒçš„å†å²æ¡ˆä¾‹æ•°é‡
            
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        # è·å–å†å²æ¡ˆä¾‹
        history = self.query_similar_problems(current_problem, model_type, limit)
        
        if not history:
            return {
                "status": "no_history",
                "message": "æœªæ‰¾åˆ°ç›¸ä¼¼å†å²æ¡ˆä¾‹",
                "recommendations": ["è¿™æ˜¯æ–°ç±»å‹çš„é—®é¢˜", "å»ºè®®è®°å½•æœ¬æ¬¡å¤„ç†ç»“æœä¾›æœªæ¥å‚è€ƒ"]
            }
        
        # ç»Ÿè®¡å†å²æˆåŠŸç‡
        successful = [h for h in history if h.get("success")]
        success_rate = len(successful) / len(history) if history else 0
        
        # æå–å¸¸è§æ¨¡å¼
        common_findings = {}
        for h in history:
            for finding in h.get("key_findings", []):
                if finding not in common_findings:
                    common_findings[finding] = 0
                common_findings[finding] += 1
        
        # æ’åºå¹¶è·å–æœ€å¸¸è§çš„å‘ç°
        sorted_findings = sorted(common_findings.items(), key=lambda x: x[1], reverse=True)
        
        return {
            "status": "comparison_complete",
            "history_count": len(history),
            "historical_success_rate": success_rate,
            "common_patterns": [f for f, _ in sorted_findings[:5]],
            "previous_approaches": [h.get("output_summary", "") for h in history[:3]],
            "recommendations": self._generate_recommendations(history, success_rate)
        }
    
    def _generate_recommendations(self, history: List[Dict], success_rate: float) -> List[str]:
        """åŸºäºå†å²ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        if success_rate > 0.7:
            recommendations.append("å†å²æˆåŠŸç‡è¾ƒé«˜ï¼Œå¯ä»¥å‚è€ƒä¹‹å‰çš„æˆåŠŸæ¨¡å¼")
        elif success_rate < 0.3:
            recommendations.append("å†å²æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®å°è¯•ä¸åŒçš„æ–¹æ³•")
        else:
            recommendations.append("å†å²æˆåŠŸç‡ä¸­ç­‰ï¼Œå»ºè®®ç»“åˆå¤šç§æ–¹æ³•")
        
        if history:
            first_history = history[0]
            if first_history.get("user_rating"):
                recommendations.append(f"ç”¨æˆ·å¯¹è¯¥ç±»é—®é¢˜çš„å†å²è¯„åˆ†: {first_history['user_rating']}/5")
        
        return recommendations
    
    def get_memory_summary(self) -> str:
        """è·å–è®°å¿†ç³»ç»Ÿæ‘˜è¦"""
        index = self._load_index()
        
        lines = [
            "ğŸ§  æ€ç»´æ¨¡å‹è®°å¿†ç³»ç»Ÿæ‘˜è¦",
            "=" * 50,
            f"",
            f"ğŸ“Š å¿«ç…§æ€»æ•°: {index['total_snapshots']}",
            f"ğŸ“ˆ æˆåŠŸç‡: {index['by_success']['success']}/{index['by_success']['success'] + index['by_success']['failed']}",
        ]
        
        if index["by_type"]:
            lines.append(f"")
            lines.append(f"ğŸ“‚ æŒ‰æ¨¡å‹ç±»å‹:")
            for model_type, count in sorted(index["by_type"].items(), key=lambda x: x[1], reverse=True):
                lines.append(f"   â€¢ {model_type}: {count}æ¬¡")
        
        if index.get("avg_rating", 0) > 0:
            lines.extend([
                f"",
                f"â­ å¹³å‡è¯„åˆ†: {index['avg_rating']:.1f}/5"
            ])
        
        lines.extend([
            f"",
            f"ğŸ• æœ€åæ›´æ–°: {index['last_updated'][:19] if index['last_updated'] else 'æœªçŸ¥'}",
        ])
        
        return "\n".join(lines)


def get_thinking_memory() -> ThinkingMemory:
    """è·å–æ€ç»´è®°å¿†å®ä¾‹"""
    return ThinkingMemory()
